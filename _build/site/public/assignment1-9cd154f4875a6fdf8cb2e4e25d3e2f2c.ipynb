{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "## CSci 39542: Introduction to Data Science\n",
    "\n",
    "**Department of Computer Science, Hunter College, City University of New York**  \n",
    "**Spring 2026**\n",
    "\n",
    "---\n",
    "\n",
    "All students must join the course's gradescope using the provided access code: **PK36RW**. Students must verify that their Gradescope account has their full name and CUNY email address in the account settings (Account > Edit Account).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Unless otherwise noted, programming assignments are submitted on the course's Gradescope site and are written in Python. The autograders expect a `.py` file and do not accept iPython notebooks. Also, to receive full credit, the code should be compatible with Python 3.6 (the default for the Gradescope autograders) and written in good style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get full credit for a program, the file must include in the opening comment: \n",
    "\n",
    "- Your name, as it appears in your Gradescope registration.\n",
    "- The email you are using for Gradescope.\n",
    "- A list of any resources you used for the program. Include classmates and tutors that you worked with, along with any websites or tutorials that you used. If you used no resources (other than the class notes and textbooks), then you should include the line: \"No resources used.\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For example, for the student, Thomas Hunter, the opening comment of his first program might be: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Name:  Thomas Hunter\n",
    "Email: thomas.hunter1870@hunter.cuny.edu\n",
    "Resources:  Used python.org as a reminder of Python 3 print statements.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and then followed by his Python program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good style accounts for 5% of the program grade. We are following the standard [PEP 8](https://peps.python.org/pep-0008/) style guide for Python code. As part of the autograder scripts, your program is run through a static code analyser (aka a \"linter\").\n",
    "\n",
    "* We are using [Pylint](https://pylint.pycqa.org/) which reports warnings and errors, including message codes, and then scores the code on a scale from 0 to 10. We multiply your percentage score by 2 (5% of the total) to determine the style grade. For example, if pylint scores your program 9, your style grade is (9/10) * 2 = 1.8.\n",
    "* See the [Pylint](https://pylint.pycqa.org/) website for tutorials, FAQ's and standard warnings.\n",
    "* [PEP 8](https://peps.python.org/pep-0008/) specifies that variable names should be at least 3 characters long, but it is long standing convention that DataFrames are named df. For the autograders, we have included df in the \"good-names\" that are accepted. This can be done locally with .pylintrc files or using the command-line option `pylint --good-names=df`.\n",
    "* Most IDE's have linting available: for example, see [pylint in PyCharm](https://plugins.jetbrains.com/plugin/11084-pylint) and [Linting Python in VSCode](https://code.visualstudio.com/docs/python/linting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program 1: School Counts\n",
    "*Due midnight, Wednesday, 17 September.*\n",
    "\n",
    "**Learning Objective:** *Refresh students' knowledge of dictionaries and string functions of core Python, use constant models, and build competency with open source data portals.*\n",
    "\n",
    "**Available Libraries:** *Core Python 3.6+ only.*\n",
    "\n",
    "**Data Sources:** NYC Open Data:\n",
    "- [2021 DOE High School Directory](https://data.cityofnewyork.us/Education/2021-DOE-High-School-Directory/8b6c-7uty)\n",
    "- [2020 DOE High School Directory](https://data.cityofnewyork.us/Education/2020-DOE-High-School-Directory/23z9-6uk9)\n",
    "- [2019 DOE High School Directory](https://data.cityofnewyork.us/Education/2019-DOE-High-School-Directory/uq7m-95z8)\n",
    "\n",
    "**Sample Datasets:**\n",
    "- [2021_DOE_High_School_Directory_SI.csv](../attachments/2021_DOE_High_School_Directory_SI.csv) (Staten Island schools)\n",
    "- [2020_DOE_High_School_Directory_late_start.csv](../attachments/2020_DOE_High_School_Directory_late_start.csv) (schools with 9am start times in 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NYC OpenData\n",
    "\n",
    "Much of NYC agency data is publicly available at [NYC Open Data](https://opendata.cityofnewyork.us). We will focus on high schools maintained by the Department of Education."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example Data](../attachments/program_1_open_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data for 2021 is available at: \n",
    "* [https://data.cityofnewyork.us/Education/2021-DOE-High-School-Directory/8b6c-7uty](https://data.cityofnewyork.us/Education/2021-DOE-High-School-Directory/8b6c-7uty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is quite large (1.7MB), let's filter the data set to just be high schools in Staten Island:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Click on the OpenData link above. Click on the `Actions` button. From the dropdown, choose `Query data`. \n",
    "* In the `Filter` region (either at the bottom of the page or on the right-hand side, depending on your browser window size), choose `Select a column to filter` and choose `borocode`.  \n",
    "* Change the `is one of` to `is`.  \n",
    "* In the search box, put `R` (for Richmond County or Staten Island).  \n",
    "* Click the `Apply` button. It will take a few seconds (it's a large file) but the rows filtered will be all the Staten Island high schools.  \n",
    "* To download the file:  \n",
    "    - Click on the `Export` button.  \n",
    "    - Under `Download`, choose `CSV`.  \n",
    "    - The download will begin automatically (files are usually stored in the `Downloads` folder).  \n",
    "* Move your CSV file to the directory where you save your programs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the Staten Island data set, create several other datasets for testing locally and to practice using the filtering on NYC OpenData. \n",
    "\n",
    "Most of the data at NYC OpenData is stored in `comma-separated-values` or CSV format. The first row is usually the column names for the data, separated by commas. Each entry is in its own row, separated by commas. For example, the first 5 rows of entire 2021 dataset are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    dbn,school_name,borocode,url,overview_paragraph,diversity_in_admissions,diadetails,\n",
    "    school_10th_seats,academicopportunities1,academicopportunities2,academicopportunities3,\n",
    "    academicopportunities4,academicopportunities5,academicopportunities6,ell_programs,\n",
    "    language_classes,advancedplacement_courses,diplomaendorsements,neighborhood,shared_space,\n",
    "    campus_name,building_code,location,phone_number,fax_number,school_email,website,\n",
    "    recruitment_website,sqr_website,subway,bus,gradespan,finalgrades,total_students,\n",
    "    freshmanschedule,start_time,end_time,addtl_info1,extracurricular_activities,\n",
    "    psal_sports_boys,psal_sports_girls,psal_sports_coed,school_sports,graduation_rate,\n",
    "    pct_stu_safe,attendance_rate,pct_stu_enough_variety,college_career_rate,girls,boys,pbat,\n",
    "    international,specialized,transfer,ptech,earlycollege,school_accessibility_description,\n",
    "    program1,program2,program3,program4,program5,..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " For this assignment, we are going to focus on the `overview_paragraph` column that contains a short description of each school. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data\n",
    "\n",
    "*Note: for this assignment, we are **not** using Pandas. We will recap Pandas in Lecture 2 and use it in the next assignment, Program 2. Instead, we're going to use the built-in file and string I/O in Python. If you're rusty on Python, see the chapters in the CSci 127 textbook: [DS 100: Section 13.1](http://learningds.org/ch/13/text_strings.html) (String Methods) and [Think CS: Chapter 11](https://runestone.academy/ns/books/published//thinkcspy/Files/toctree.html) (Files).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Programming assignments are submitted as a single `.py` file. If you use multiple files or notebooks, convert your program to a single `.py` file to submit to the autograder. For this first program, we have set up a template (see [p1_template.py](../attachments/p1_template.py) and also included at the end of this page). Edit the template to include your name, email, and resources and submit to Gradescope to make sure everything works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have downloaded some test data sets to your device, the next thing to do is format the data to be usable for analysis. Add the following function to your file:\n",
    "\n",
    "* `extract_overviews(file_name)`: Opens the `file_name` and from each line of the file, keeps the overview description of the school (the fifth \"column\": `overview_paragraph`). Returns a list of the overviews. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, starting with the Staten Island data set (and using the textwrap package to print prettily): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '2021_DOE_High_School_Directory_SI.csv'\n",
    "si_overviews = extract_overviews(file_name)\n",
    "print(f\"Number of SI overviews: {len(si_overviews)}. The the last one is:\\n\")\n",
    "print(textwrap.fill(si_overviews[-1],80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gives the output: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Number of SI overviews: 11. The the last one is:\n",
    "\n",
    "SI Technical High School provides a robust liberal arts curriculum including\n",
    "courses in science, technology, engineering, arts, and mathematics (STEAM) and a\n",
    "cutting edge Career and Technical Education (CTE) program. All students take an\n",
    "Intensive Writing course and English and Language Arts curriculum to prepare\n",
    "them for Advanced Placement (AP) Language and AP Literature and Composition.\n",
    "Students take four years of mathematics, a variety of STEM and AP courses,\n",
    "graduate with at least two or three AP Social Studies courses, and take three\n",
    "years of the Russian language with an optional fourth year of a second language\n",
    "via a blended learning program. All ninth grade students receive a computer to\n",
    "use in school and to take home.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the fuction on the late start data set (schools that start 9am or later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_name = '2020_DOE_High_School_Directory_late_start.csv'\n",
    "late_overviews = extract_overviews(late_name)\n",
    "print(f\"\\n\\nNumber of late start overviews: {len(late_overviews)}. The the last one is:\\n\")\n",
    "print(textwrap.fill(late_overviews[-1],80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gives the output: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```txt\n",
    "Number of late start overviews: 30. The the last one is:\n",
    "\n",
    "The mission of the Bronx International High School is to empower our students to\n",
    "become active participants in today's interdependent and diverse world. We\n",
    "accomplish this by helping enhance our students' cultural awareness, English and\n",
    "native language proficiencies, and intellectual and collaborative abilities. We\n",
    "are dedicated to serving the academic and social needs of recently immigrated\n",
    "young people and their families. By critically analyzing and responding to\n",
    "complex world issues, students achieve academic, personal, and professional\n",
    "success as they become advocates for themselves and their communities.  \n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have written your function, test it locally on the small test files. When it works, upload to Gradescope. Given the size of the files that we evaluate your code, you will find it much faster to develop and test the code in your IDE than debugging and testing in Gradescope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constant Model\n",
    "\n",
    "The first model that we will use is the **constant model** which predicts the same (constant) values for all inputs. We saw examples in Lecture 1 and [Chapter 4](https://learningds.org/ch/04/modeling_intro.html) with modeling restaurant tips and bus lateness. We are going to build two models: one that predicts the length of the overview paragraphs (in number of characters) and a second that predicts the number of sentences (using the number of periods as a proxy for the number of sentences). To do that, we will use some of our datasets to compute values and determine a good constant for each one. Since the datasets can be quite large, instead of storing the numeric values in a list, we will use a dictionary that keeps count of each time a value is seen (if you're rusty on dictionaries, see [Think CS: Chapter 12](https://runestone.academy/ns/books/published//thinkcspy/Dictionaries/toctree.html) or [Learning Python 3 from Scratch](https://www.educative.io/courses/learn-python-3-from-scratch)).\n",
    "\n",
    "Implement the functions below. In the real world, you would likely combine these into one function, but we are implementing them separately to make partial credit and testing easier:\n",
    "\n",
    "- `count_lengths(overview_list)`: For each element of the `overview_list`, the function computes the length (the number of characters in the string). The results are stored in a dictionary of length occurrences where the keys are the lengths seen in `overview_list` and the values are the number of times each length occurs. Returns the dictionary of length occurrences.  \n",
    "- `count_num_sentences(overview_list)`: For each element of the `overview_list`, the function computes the number of periods (`.`) (as a proxy for the number of sentences). The results are stored in a dictionary of occurrences where the keys are the number of periods seen in `overview_list` and the values are the number of times each occurs. Returns the dictionary of occurrences.  \n",
    "- `compute_mean(counts)`: Computes the mean (average) of `counts` dictionary weighting each key that occurs by its value (e.g. if the key of 10 has value 8, then the 10 showed up 8 times and adds 10*8 to the computation of the average). Returns the mean.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing our example for the 11 Staten Island high schools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_len_counts = count_lengths(si_overviews)\n",
    "print(f\"The {sum(si_len_counts.values())} entries have lengths:\")    \n",
    "print(si_len_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The 11 entries have lengths:\n",
    "{729: 1, 738: 1, 681: 1, 435: 1, 536: 1, 732: 2, 741: 1, 623: 1, 564: 1, 735: 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note that two entries have the same length.\n",
    "\n",
    "Continuing our late start high school example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_len_counts = count_lengths(late_overviews)\n",
    "print(f\"The {sum(late_len_counts.values())} entries have lengths:\")    \n",
    "print(late_len_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gives the output: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The 30 entries have lengths:\n",
    "{38: 1, 634: 1, 528: 1, 743: 1, 748: 1, 385: 1, 753: 1, 27: 1, 684: 1, 512: 1, 680: 1, 477: 1, 722: 1, 741: 1, 106: 1, 739: 1, 732: 1, 700: 1, 750: 1, 551: 1, 733: 1, 399: 1, 22: 1, 679: 1, 723: 1, 31: 1, 73: 1, 710: 1, 655: 1, 616: 1}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can similarly compute the number of sentences: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_dots_counts = count_sentences(si_overviews)\n",
    "print(f\"The {sum(si_dots_counts.values())} entries have lengths:\")    \n",
    "print(si_dots_counts)\n",
    "late_dots_counts = count_sentences(late_overviews)\n",
    "print(f\"The {sum(late_dots_counts.values())} entries have lengths:\")    \n",
    "print(late_dots_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gives the output: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The 11 entries have lengths:\n",
    "{4: 6, 7: 2, 3: 2, 5: 1}\n",
    "The 30 entries have lengths:\n",
    "{0: 5, 7: 2, 3: 3, 4: 11, 5: 3, 2: 1, 6: 5}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these small examples, the overall lengths of the paragraphs were different, but the number of sentences were much more concentrated.\n",
    "\n",
    "We can compute the means as well: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_len_mean = compute_mean(si_len_counts)\n",
    "si_dots_mean = compute_mean(si_dots_counts)\n",
    "print(f\"Staten Island high schools overviews had an average of {si_len_mean:.2f} \\\n",
    "characters in {si_dots_mean:.2f} sentences.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gives the output: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Staten Island high schools overviews had an average of 658.73 characters in 4.45 sentences.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating Our Model\n",
    "\n",
    "The next part of program evaluates how well our constant models do at prediction. We will use a loss function, mean squared error, introduced in Lecture 1 and Section 4.2.\n",
    "\n",
    "* `compute_mse(theta, counts)`: This function takes two inputs:\n",
    "    * `theta`: a numeric value.\n",
    "    * `counts`: a dictionary where the keys are numerical and the values are the number of occurrences. \n",
    "    \n",
    "    Computes the Mean Squared Error of the parameter `theta` and a dictionary, `counts`. See [Section 4.2: Modeling Loss Functions](https://learningds.org/ch/04/modeling_loss_functions.html) where this function is implemented using `numpy` and assumes the values are in a list or Series. **Note:** `numpy` is not one of the libraries for this assignment and your function should compute MSE **without using numpy**. Returns the MSE.\n",
    "\n",
    "Continuing our example of number of sentences in an overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_dots_mean = compute_mean(late_dots_counts)\n",
    "print(f\"The mean for number of sentences in SI descriptions is {late_dots_mean}.\")\n",
    "losses = []\n",
    "for theta in range(10):\n",
    "    loss = compute_mse(theta, late_dots_counts)\n",
    "    print(f\"For theta = {theta}, MSE loss is {loss:.2f}.\")\n",
    "    losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gives the output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The mean for number of sentences in SI descriptions is 3.8.\n",
    "For theta = 0, MSE loss is 18.67.\n",
    "For theta = 1, MSE loss is 12.07.\n",
    "For theta = 2, MSE loss is 7.47.\n",
    "For theta = 3, MSE loss is 4.87.\n",
    "For theta = 4, MSE loss is 4.27.\n",
    "For theta = 5, MSE loss is 5.67.\n",
    "For theta = 6, MSE loss is 9.07.\n",
    "For theta = 7, MSE loss is 14.47.\n",
    "For theta = 8, MSE loss is 21.87.\n",
    "For theta = 9, MSE loss is 31.27.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The smallest losses are near the mean of 3.8. We can also visualize the lengths (as a histogram in blue) and the loss function in terms of theta (in red) using matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.bar(late_dots_counts.keys(),late_dots_counts.values())\n",
    "plt.plot(losses, color='r')\n",
    "plt.title('Sentences in late overviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![late](../attachments/late_sentences.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This suggests that the best constant model for sentence length is 3.8. Let's look at the Staten Island data (generated similarly to above) which has mean 4.45: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![si_sent](../attachments/si_sentences.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the graph of the loss function (red line), it's minimized closer to 4.5 than 3.8. So, our constant model built on late-starting schools does not do as well for predicting sentences in SI descriptions. We will see in later lectures that the MSE loss function is minimized for the mean value (and that other loss functions achieve their minimum values at different values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Code\n",
    "\n",
    "Each programming assignment includes functions that test that your code works (a \"test suite\"). We will first build these in core Python, and in future assignments (Programs 3-6), will introduce standard testing packages.\n",
    "\n",
    "Your program should include the functions below that test if your functions above perform correctly. Each of these functions takes a function as an argument. You can write them in any order, but we have placed them easiest to hardest below:\n",
    "\n",
    "- `test_compute_mean(mean_fnc=compute_mean)`:\n",
    "\n",
    "  This test function takes one input:\n",
    "\n",
    "  - `mean_fnc`: a function that takes one input parameter (a dictionary of counts) and returns a numeric value. It has a default value of `compute_mean`.\n",
    "\n",
    "  Returns `True` if the function performs correctly (computes the mean) and `False` otherwise.\n",
    "\n",
    "  _Hint: we will test your function with three different functions: the correct one, one that returns the mean of the keys (i.e., doesn't use the counts when computing the mean), and one that always returns 42._\n",
    "\n",
    "- `test_mse(loss_fnc=compute_mse)`:\n",
    "\n",
    "  This test function takes one input:\n",
    "\n",
    "  - `mse_fnc`: a function that takes in two input parameters (a numeric value and a dictionary of counts) and returns a numeric value. It has a default value of `compute_mse`.\n",
    "\n",
    "  Returns `True` if the function performs correctly (computes mean squared error) and `False` otherwise.\n",
    "\n",
    "  _Hint: we will test your function with three different functions: the correct one, one that returns the mean squared error of the keys (doesn't use the counts), and one that always returns 42._\n",
    "\n",
    "- `test_count_lengths(count_fnc=count_lengths)`:\n",
    "\n",
    "  This test function takes one input:\n",
    "\n",
    "  - `count_fnc`: a function that takes one input parameter (a list of strings) and returns a dictionary. It has a default value of `count_lengths`.\n",
    "\n",
    "  Returns `True` if the function performs correctly (creates a dictionary of counts of occurrences of lengths) and `False` otherwise.\n",
    "\n",
    "  _Hint: we will test your function with three different functions: the correct one, one that returns the number of strings in the list, and one that always returns a dictionary with the key:value pair 42:42._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying first on the correct function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'test_compute_mean(compute_mean) returns {test_compute_mean(compute_mean)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " gives the output: \n",
    "\n",
    " `test_compute_mean(compute_mean) returns True.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Continuing our example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'test_compute_mean( lambda x : 42 ) returns {test_compute_mean(lambda x : 42)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " gives the output: \n",
    " \n",
    " `test_compute_mean( lambda x : 42 ) returns False.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The `lambda` in Python allows you to write small anonymous functions (see [Python Docs 4.8.6 Lambda Expressions](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions) for more details). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Program Template\n",
    "\n",
    "For this first program, we have included a template, [p1_template.py](../attachments/p1_template.py) to use to get started. It includes function stubs and some testing in a (conditionally executed) main function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Name: YOUR NAME HERE (as it appears in Gradescope)\n",
    "  Email: YOUR EMAIL HERE (as it appears in Gradescope)\n",
    "  Resources:  ANY RESOURCES YOU USED\n",
    "\"\"\"\n",
    "import textwrap\n",
    "\n",
    "def extract_overviews(file_name):\n",
    "  \"\"\"\n",
    "  Opens the file_name and from each line of the file, keeps the overview\n",
    "  description of the school (the fifth \"column\": overview_paragraph.\n",
    "  Returns a list of the paragraphs.\n",
    "  \"\"\"\n",
    "\n",
    "  #Placeholder-- replace with your code\n",
    "  lst = []\n",
    "  return lst\n",
    "\n",
    "def count_lengths(overview_list):\n",
    "  \"\"\"\n",
    "  For each element of the overview_list, computes the length (# of characters).\n",
    "  Returns the dictionary of length occurrences.\n",
    "  \"\"\"\n",
    "\n",
    "  #Placeholder-- replace with your code\n",
    "  counts = {}\n",
    "  return counts\n",
    "\n",
    "def count_sentences(overview_list):\n",
    "  \"\"\"\n",
    "  For each element of the overview_list, computes the number of periods \n",
    "  (as a proxy for the number of sentences).\n",
    "  Returns the dictionary of occurrences.\n",
    "  \"\"\"\n",
    "\n",
    "  #Placeholder-- replace with your code\n",
    "  counts = {}\n",
    "  return counts\n",
    "\n",
    "\n",
    "def compute_mean(counts):\n",
    "  \"\"\"\n",
    "  Computes the mean of counts dictionary, weighting each key that occurs by its value.\n",
    "  Returns the mean. \n",
    "  \"\"\"\n",
    "\n",
    "  #Placeholder-- replace with your code\n",
    "  mean = 0\n",
    "  return mean\n",
    "\n",
    "\n",
    "def compute_mse(theta, counts):\n",
    "  \"\"\"\n",
    "  Computes the Mean Squared Error of the parameter theta and a dictionary, counts.\n",
    "  Returns the MSE.\n",
    "  \"\"\"\n",
    "\n",
    "  #Placeholder-- replace with your code\n",
    "  mse = 0\n",
    "\n",
    "  return mse\n",
    "\n",
    "def test_compute_mean(mean_fnc=compute_mean):\n",
    "  \"\"\"\n",
    "  Returns True if the mean_fnc performs correctly\n",
    "  (e.g. computes weighted mean of inputted dictionary) and False otherwise. \n",
    "  \"\"\"\n",
    "\n",
    "  #Placeholder-- replace with your code\n",
    "  correct = True\n",
    "  return correct\n",
    "\n",
    "\n",
    "def test_mse(mse_fnc=compute_mse):\n",
    "  \"\"\"\n",
    "  Returns True if the extract_fnc performs correctly\n",
    "  (e.g. computes mean squared error) and False otherwise.\n",
    "  \"\"\"\n",
    "\n",
    "  #Placeholder-- replace with your code\n",
    "  correct = True\n",
    "  return correct\n",
    "\n",
    "def test_count_lengths(counts_fnc=count_lengths):\n",
    "  \"\"\"\n",
    "  Returns True if the counts_fnc performs correctly\n",
    "  (e.g. counts lengths of overviews and stores in dictionary) & False otherwise.\n",
    "  \"\"\"\n",
    "\n",
    "  #Placeholder-- replace with your code\n",
    "\n",
    "  correct = True\n",
    "  return correct\n",
    "\n",
    "\n",
    "def main():\n",
    "  \"\"\"\n",
    "  Some examples of the functions in use:\n",
    "  \"\"\"\n",
    "\n",
    "  ###Extracts the overviews from the data files:\n",
    "  file_name = 'fall23/program01/2021_DOE_High_School_Directory_SI.csv'\n",
    "  si_overviews = extract_overviews(file_name)\n",
    "  print(f\"Number of SI overviews: {len(si_overviews)}. The the last one is:\\n\")\n",
    "  #Using textwrap for prettier printing:\n",
    "  print(textwrap.fill(si_overviews[-1],80))\n",
    "\n",
    "  late_name = 'fall23/program01/2020_DOE_High_School_Directory_late_start.csv'\n",
    "  late_overviews = extract_overviews(late_name)\n",
    "  print(f\"\\n\\nNumber of late start overviews: {len(late_overviews)}. The the last one is:\\n\")\n",
    "  print(textwrap.fill(late_overviews[-1],80))\n",
    "\n",
    "  ###Computing counts and means:\n",
    "  si_len_counts = count_lengths(si_overviews)\n",
    "  print(f\"The {sum(si_len_counts.values())} entries have lengths:\")\n",
    "  print(si_len_counts)\n",
    "  late_len_counts = count_lengths(late_overviews)\n",
    "  print(f\"The {sum(late_len_counts.values())} entries have lengths:\")\n",
    "  print(late_len_counts)\n",
    "\n",
    "  si_dots_counts = count_sentences(si_overviews)\n",
    "  print(f\"The {sum(si_dots_counts.values())} entries have lengths:\")\n",
    "  print(si_dots_counts)\n",
    "  late_dots_counts = count_sentences(late_overviews)\n",
    "  print(f\"The {sum(late_dots_counts.values())} entries have lengths:\")\n",
    "  print(late_dots_counts)\n",
    "\n",
    "  si_len_mean = compute_mean(si_len_counts)\n",
    "  si_dots_mean = compute_mean(si_dots_counts)\n",
    "  print(f\"Staten Island high schools overviews had an average of {si_len_mean:.2f}\\\n",
    "characters in {si_dots_mean:.2f} sentences.\")\n",
    "\n",
    "  ###Computing MSE:\n",
    "  late_dots_mean = compute_mean(late_dots_counts)\n",
    "  print(f\"The mean for number of sentences in SI descriptions is {late_dots_mean}.\")\n",
    "  losses = []\n",
    "  for theta in range(10):\n",
    "      loss = compute_mse(theta,late_dots_counts)\n",
    "      print(f\"For theta = {theta}, MSE loss is {loss:.2f}.\")\n",
    "      losses.append(loss)\n",
    "\n",
    "  losses = []\n",
    "  for theta in range(10):\n",
    "      loss = compute_mse(theta,si_dots_counts)\n",
    "      print(f\"For theta = {theta}, MSE loss is {loss:.2f}.\")\n",
    "      losses.append(loss)\n",
    "\n",
    "  ###Testing\n",
    "  #Trying first on the correct function:\n",
    "  print(f'test_compute_mean(compute_mean) returns {test_compute_mean(compute_mean)}.')\n",
    "  #Trying on a function that returns 42 no matter what the output:\n",
    "  print(f'test_compute_mean( lambda x : 42 ) returns {test_compute_mean(lambda x : 42)}.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
