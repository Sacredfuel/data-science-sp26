{"version":"1","records":[{"hierarchy":{"lvl1":"Assignment 1"},"type":"lvl1","url":"/assignment1","position":0},{"hierarchy":{"lvl1":"Assignment 1"},"content":"","type":"content","url":"/assignment1","position":1},{"hierarchy":{"lvl1":"Assignment 1","lvl2":"CSci 39542: Introduction to Data Science"},"type":"lvl2","url":"/assignment1#csci-39542-introduction-to-data-science","position":2},{"hierarchy":{"lvl1":"Assignment 1","lvl2":"CSci 39542: Introduction to Data Science"},"content":"Department of Computer Science, Hunter College, City University of New YorkSpring 2026\n\nAll students must join the course’s gradescope using the provided access code: PK36RW. Students must verify that their Gradescope account has their full name and CUNY email address in the account settings (Account > Edit Account).\n\nUnless otherwise noted, programming assignments are submitted on the course’s Gradescope site and are written in Python. The autograders expect a .py file and do not accept iPython notebooks. Also, to receive full credit, the code should be compatible with Python 3.6 (the default for the Gradescope autograders) and written in good style.\n\nTo get full credit for a program, the file must include in the opening comment:\n\nYour name, as it appears in your Gradescope registration.\n\nThe email you are using for Gradescope.\n\nA list of any resources you used for the program. Include classmates and tutors that you worked with, along with any websites or tutorials that you used. If you used no resources (other than the class notes and textbooks), then you should include the line: “No resources used.”\n\nFor example, for the student, Thomas Hunter, the opening comment of his first program might be:\n\n\"\"\"\nName:  Thomas Hunter\nEmail: thomas.hunter1870@hunter.cuny.edu\nResources:  Used python.org as a reminder of Python 3 print statements.\n\"\"\"\n\n\n\nand then followed by his Python program.\n\nGood style accounts for 5% of the program grade. We are following the standard \n\nPEP 8 style guide for Python code. As part of the autograder scripts, your program is run through a static code analyser (aka a “linter”).\n\nWe are using \n\nPylint which reports warnings and errors, including message codes, and then scores the code on a scale from 0 to 10. We multiply your percentage score by 2 (5% of the total) to determine the style grade. For example, if pylint scores your program 9, your style grade is (9/10) * 2 = 1.8.\n\nSee the \n\nPylint website for tutorials, FAQ’s and standard warnings.\n\nPEP 8 specifies that variable names should be at least 3 characters long, but it is long standing convention that DataFrames are named df. For the autograders, we have included df in the “good-names” that are accepted. This can be done locally with .pylintrc files or using the command-line option pylint --good-names=df.\n\nMost IDE’s have linting available: for example, see \n\npylint in PyCharm and \n\nLinting Python in VSCode.\n\n","type":"content","url":"/assignment1#csci-39542-introduction-to-data-science","position":3},{"hierarchy":{"lvl1":"Assignment 1","lvl3":"Program 1: School Counts","lvl2":"CSci 39542: Introduction to Data Science"},"type":"lvl3","url":"/assignment1#program-1-school-counts","position":4},{"hierarchy":{"lvl1":"Assignment 1","lvl3":"Program 1: School Counts","lvl2":"CSci 39542: Introduction to Data Science"},"content":"Due date TBD\n\nLearning Objective: Refresh students’ knowledge of dictionaries and string functions of core Python, use constant models, and build competency with open source data portals.\n\nAvailable Libraries: Core Python 3.6+ only.\n\nData Sources: NYC Open Data:\n\n2021 DOE High School Directory\n\n2020 DOE High School Directory\n\n2019 DOE High School Directory\n\nSample Datasets:\n\n2021​_DOE​_High​_School​_Directory​_SI​.csv (Staten Island schools)\n\n2020​_DOE​_High​_School​_Directory​_late​_start​.csv (schools with 9am start times in 2020)\n\n","type":"content","url":"/assignment1#program-1-school-counts","position":5},{"hierarchy":{"lvl1":"Assignment 1","lvl4":"NYC OpenData","lvl3":"Program 1: School Counts","lvl2":"CSci 39542: Introduction to Data Science"},"type":"lvl4","url":"/assignment1#nyc-opendata","position":6},{"hierarchy":{"lvl1":"Assignment 1","lvl4":"NYC OpenData","lvl3":"Program 1: School Counts","lvl2":"CSci 39542: Introduction to Data Science"},"content":"Much of NYC agency data is publicly available at \n\nNYC Open Data. We will focus on high schools maintained by the Department of Education.\n\n\n\nThe raw data for 2021 is available at:\n\nhttps://​data​.cityofnewyork​.us​/Education​/2021​-DOE​-High​-School​-Directory​/8b6c​-7uty\n\nSince it is quite large (1.7MB), let’s filter the data set to just be high schools in Staten Island:\n\nClick on the OpenData link above. Click on the Actions button. From the dropdown, choose Query data.\n\nIn the Filter region (either at the bottom of the page or on the right-hand side, depending on your browser window size), choose Select a column to filter and choose borocode.\n\nChange the is one of to is.\n\nIn the search box, put R (for Richmond County or Staten Island).\n\nClick the Apply button. It will take a few seconds (it’s a large file) but the rows filtered will be all the Staten Island high schools.\n\nTo download the file:\n\nClick on the Export button.\n\nUnder Download, choose CSV.\n\nThe download will begin automatically (files are usually stored in the Downloads folder).\n\nMove your CSV file to the directory where you save your programs.\n\nIn addition to the Staten Island data set, create several other datasets for testing locally and to practice using the filtering on NYC OpenData.\n\nMost of the data at NYC OpenData is stored in comma-separated-values or CSV format. The first row is usually the column names for the data, separated by commas. Each entry is in its own row, separated by commas. For example, the first 5 rows of entire 2021 dataset are:\n\ndbn,school_name,borocode,url,overview_paragraph,diversity_in_admissions,diadetails,\nschool_10th_seats,academicopportunities1,academicopportunities2,academicopportunities3,\nacademicopportunities4,academicopportunities5,academicopportunities6,ell_programs,\nlanguage_classes,advancedplacement_courses,diplomaendorsements,neighborhood,shared_space,\ncampus_name,building_code,location,phone_number,fax_number,school_email,website,\nrecruitment_website,sqr_website,subway,bus,gradespan,finalgrades,total_students,\nfreshmanschedule,start_time,end_time,addtl_info1,extracurricular_activities,\npsal_sports_boys,psal_sports_girls,psal_sports_coed,school_sports,graduation_rate,\npct_stu_safe,attendance_rate,pct_stu_enough_variety,college_career_rate,girls,boys,pbat,\ninternational,specialized,transfer,ptech,earlycollege,school_accessibility_description,\nprogram1,program2,program3,program4,program5,...\n\nFor this assignment, we are going to focus on the overview_paragraph column that contains a short description of each school.\n\n","type":"content","url":"/assignment1#nyc-opendata","position":7},{"hierarchy":{"lvl1":"Assignment 1","lvl4":"Cleaning Data","lvl3":"Program 1: School Counts","lvl2":"CSci 39542: Introduction to Data Science"},"type":"lvl4","url":"/assignment1#cleaning-data","position":8},{"hierarchy":{"lvl1":"Assignment 1","lvl4":"Cleaning Data","lvl3":"Program 1: School Counts","lvl2":"CSci 39542: Introduction to Data Science"},"content":"Note: for this assignment, we are not using Pandas. We will recap Pandas in Lecture 2 and use it in the next assignment, Program 2. Instead, we’re going to use the built-in file and string I/O in Python. If you’re rusty on Python, see the chapters in the CSci 127 textbook: \n\nDS 100: Section 13.1 (String Methods) and \n\nThink CS: Chapter 11 (Files).\n\nProgramming assignments are submitted as a single .py file. If you use multiple files or notebooks, convert your program to a single .py file to submit to the autograder. For this first program, we have set up a template (see \n\np1_template.py and also included at the end of this page). Edit the template to include your name, email, and resources and submit to Gradescope to make sure everything works.\n\nOnce you have downloaded some test data sets to your device, the next thing to do is format the data to be usable for analysis. Add the following function to your file:\n\nextract_overviews(file_name): Opens the file_name and from each line of the file, keeps the overview description of the school (the fifth “column”: overview_paragraph). Returns a list of the overviews.\n\nFor example, starting with the Staten Island data set (and using the textwrap package to print prettily):\n\nfile_name = '2021_DOE_High_School_Directory_SI.csv'\nsi_overviews = extract_overviews(file_name)\nprint(f\"Number of SI overviews: {len(si_overviews)}. The the last one is:\\n\")\nprint(textwrap.fill(si_overviews[-1],80))\n\n\n\ngives the output:\n\nNumber of SI overviews: 11. The the last one is:\n\nSI Technical High School provides a robust liberal arts curriculum including\ncourses in science, technology, engineering, arts, and mathematics (STEAM) and a\ncutting edge Career and Technical Education (CTE) program. All students take an\nIntensive Writing course and English and Language Arts curriculum to prepare\nthem for Advanced Placement (AP) Language and AP Literature and Composition.\nStudents take four years of mathematics, a variety of STEM and AP courses,\ngraduate with at least two or three AP Social Studies courses, and take three\nyears of the Russian language with an optional fourth year of a second language\nvia a blended learning program. All ninth grade students receive a computer to\nuse in school and to take home.\n\nRunning the fuction on the late start data set (schools that start 9am or later):\n\nlate_name = '2020_DOE_High_School_Directory_late_start.csv'\nlate_overviews = extract_overviews(late_name)\nprint(f\"\\n\\nNumber of late start overviews: {len(late_overviews)}. The the last one is:\\n\")\nprint(textwrap.fill(late_overviews[-1],80))\n\n\n\ngives the output:\n\nNumber of late start overviews: 30. The the last one is:\n\nThe mission of the Bronx International High School is to empower our students to\nbecome active participants in today's interdependent and diverse world. We\naccomplish this by helping enhance our students' cultural awareness, English and\nnative language proficiencies, and intellectual and collaborative abilities. We\nare dedicated to serving the academic and social needs of recently immigrated\nyoung people and their families. By critically analyzing and responding to\ncomplex world issues, students achieve academic, personal, and professional\nsuccess as they become advocates for themselves and their communities.  \n\nOnce you have written your function, test it locally on the small test files. When it works, upload to Gradescope. Given the size of the files that we evaluate your code, you will find it much faster to develop and test the code in your IDE than debugging and testing in Gradescope.\n\n","type":"content","url":"/assignment1#cleaning-data","position":9},{"hierarchy":{"lvl1":"Assignment 1","lvl4":"Constant Model","lvl3":"Program 1: School Counts","lvl2":"CSci 39542: Introduction to Data Science"},"type":"lvl4","url":"/assignment1#constant-model","position":10},{"hierarchy":{"lvl1":"Assignment 1","lvl4":"Constant Model","lvl3":"Program 1: School Counts","lvl2":"CSci 39542: Introduction to Data Science"},"content":"The first model that we will use is the constant model which predicts the same (constant) values for all inputs. We saw examples in Lecture 1 and \n\nChapter 4 with modeling restaurant tips and bus lateness. We are going to build two models: one that predicts the length of the overview paragraphs (in number of characters) and a second that predicts the number of sentences (using the number of periods as a proxy for the number of sentences). To do that, we will use some of our datasets to compute values and determine a good constant for each one. Since the datasets can be quite large, instead of storing the numeric values in a list, we will use a dictionary that keeps count of each time a value is seen (if you’re rusty on dictionaries, see \n\nThink CS: Chapter 12 or \n\nLearning Python 3 from Scratch).\n\nImplement the functions below. In the real world, you would likely combine these into one function, but we are implementing them separately to make partial credit and testing easier:\n\ncount_lengths(overview_list): For each element of the overview_list, the function computes the length (the number of characters in the string). The results are stored in a dictionary of length occurrences where the keys are the lengths seen in overview_list and the values are the number of times each length occurs. Returns the dictionary of length occurrences.\n\ncount_num_sentences(overview_list): For each element of the overview_list, the function computes the number of periods (.) (as a proxy for the number of sentences). The results are stored in a dictionary of occurrences where the keys are the number of periods seen in overview_list and the values are the number of times each occurs. Returns the dictionary of occurrences.\n\ncompute_mean(counts): Computes the mean (average) of counts dictionary weighting each key that occurs by its value (e.g. if the key of 10 has value 8, then the 10 showed up 8 times and adds 10*8 to the computation of the average). Returns the mean.\n\nContinuing our example for the 11 Staten Island high schools:\n\nsi_len_counts = count_lengths(si_overviews)\nprint(f\"The {sum(si_len_counts.values())} entries have lengths:\")    \nprint(si_len_counts)\n\n\n\nThe 11 entries have lengths:\n{729: 1, 738: 1, 681: 1, 435: 1, 536: 1, 732: 2, 741: 1, 623: 1, 564: 1, 735: 1}\n\nNote that two entries have the same length.\n\nContinuing our late start high school example:\n\nlate_len_counts = count_lengths(late_overviews)\nprint(f\"The {sum(late_len_counts.values())} entries have lengths:\")    \nprint(late_len_counts)\n\n\n\ngives the output:\n\nThe 30 entries have lengths:\n{38: 1, 634: 1, 528: 1, 743: 1, 748: 1, 385: 1, 753: 1, 27: 1, 684: 1, 512: 1, 680: 1, 477: 1, 722: 1, 741: 1, 106: 1, 739: 1, 732: 1, 700: 1, 750: 1, 551: 1, 733: 1, 399: 1, 22: 1, 679: 1, 723: 1, 31: 1, 73: 1, 710: 1, 655: 1, 616: 1}\n\nWe can similarly compute the number of sentences:\n\nsi_dots_counts = count_sentences(si_overviews)\nprint(f\"The {sum(si_dots_counts.values())} entries have lengths:\")    \nprint(si_dots_counts)\nlate_dots_counts = count_sentences(late_overviews)\nprint(f\"The {sum(late_dots_counts.values())} entries have lengths:\")    \nprint(late_dots_counts)\n\n\n\ngives the output:\n\nThe 11 entries have lengths:\n{4: 6, 7: 2, 3: 2, 5: 1}\nThe 30 entries have lengths:\n{0: 5, 7: 2, 3: 3, 4: 11, 5: 3, 2: 1, 6: 5}\n\nFor these small examples, the overall lengths of the paragraphs were different, but the number of sentences were much more concentrated.\n\nWe can compute the means as well:\n\nsi_len_mean = compute_mean(si_len_counts)\nsi_dots_mean = compute_mean(si_dots_counts)\nprint(f\"Staten Island high schools overviews had an average of {si_len_mean:.2f} \\\ncharacters in {si_dots_mean:.2f} sentences.\")\n\n\n\ngives the output:\n\nStaten Island high schools overviews had an average of 658.73 characters in 4.45 sentences.\n\n","type":"content","url":"/assignment1#constant-model","position":11},{"hierarchy":{"lvl1":"Assignment 1","lvl4":"Evaluating Our Model","lvl3":"Program 1: School Counts","lvl2":"CSci 39542: Introduction to Data Science"},"type":"lvl4","url":"/assignment1#evaluating-our-model","position":12},{"hierarchy":{"lvl1":"Assignment 1","lvl4":"Evaluating Our Model","lvl3":"Program 1: School Counts","lvl2":"CSci 39542: Introduction to Data Science"},"content":"The next part of program evaluates how well our constant models do at prediction. We will use a loss function, mean squared error, introduced in Lecture 1 and Section 4.2.\n\ncompute_mse(theta, counts): This function takes two inputs:\n\ntheta: a numeric value.\n\ncounts: a dictionary where the keys are numerical and the values are the number of occurrences.\n\nComputes the Mean Squared Error of the parameter theta and a dictionary, counts. See \n\nSection 4.2: Modeling Loss Functions where this function is implemented using numpy and assumes the values are in a list or Series. Note: numpy is not one of the libraries for this assignment and your function should compute MSE without using numpy. Returns the MSE.\n\nContinuing our example of number of sentences in an overview:\n\nlate_dots_mean = compute_mean(late_dots_counts)\nprint(f\"The mean for number of sentences in SI descriptions is {late_dots_mean}.\")\nlosses = []\nfor theta in range(10):\n    loss = compute_mse(theta, late_dots_counts)\n    print(f\"For theta = {theta}, MSE loss is {loss:.2f}.\")\n    losses.append(loss)\n\n\n\ngives the output:\n\nThe mean for number of sentences in SI descriptions is 3.8.\nFor theta = 0, MSE loss is 18.67.\nFor theta = 1, MSE loss is 12.07.\nFor theta = 2, MSE loss is 7.47.\nFor theta = 3, MSE loss is 4.87.\nFor theta = 4, MSE loss is 4.27.\nFor theta = 5, MSE loss is 5.67.\nFor theta = 6, MSE loss is 9.07.\nFor theta = 7, MSE loss is 14.47.\nFor theta = 8, MSE loss is 21.87.\nFor theta = 9, MSE loss is 31.27.\n\nThe smallest losses are near the mean of 3.8. We can also visualize the lengths (as a histogram in blue) and the loss function in terms of theta (in red) using matplotlib:\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.bar(late_dots_counts.keys(),late_dots_counts.values())\nplt.plot(losses, color='r')\nplt.title('Sentences in late overviews')\nplt.show()\n\n\n\n\n\nThis suggests that the best constant model for sentence length is 3.8. Let’s look at the Staten Island data (generated similarly to above) which has mean 4.45:\n\n\n\nLooking at the graph of the loss function (red line), it’s minimized closer to 4.5 than 3.8. So, our constant model built on late-starting schools does not do as well for predicting sentences in SI descriptions. We will see in later lectures that the MSE loss function is minimized for the mean value (and that other loss functions achieve their minimum values at different values).\n\n","type":"content","url":"/assignment1#evaluating-our-model","position":13},{"hierarchy":{"lvl1":"Assignment 1","lvl4":"Testing Code","lvl3":"Program 1: School Counts","lvl2":"CSci 39542: Introduction to Data Science"},"type":"lvl4","url":"/assignment1#testing-code","position":14},{"hierarchy":{"lvl1":"Assignment 1","lvl4":"Testing Code","lvl3":"Program 1: School Counts","lvl2":"CSci 39542: Introduction to Data Science"},"content":"Each programming assignment includes functions that test that your code works (a “test suite”). We will first build these in core Python, and in future assignments (Programs 3-6), will introduce standard testing packages.\n\nYour program should include the functions below that test if your functions above perform correctly. Each of these functions takes a function as an argument. You can write them in any order, but we have placed them easiest to hardest below:\n\ntest_compute_mean(mean_fnc=compute_mean):\n\nThis test function takes one input:\n\nmean_fnc: a function that takes one input parameter (a dictionary of counts) and returns a numeric value. It has a default value of compute_mean.\n\nReturns True if the function performs correctly (computes the mean) and False otherwise.\n\nHint: we will test your function with three different functions: the correct one, one that returns the mean of the keys (i.e., doesn’t use the counts when computing the mean), and one that always returns 42.\n\ntest_mse(loss_fnc=compute_mse):\n\nThis test function takes one input:\n\nmse_fnc: a function that takes in two input parameters (a numeric value and a dictionary of counts) and returns a numeric value. It has a default value of compute_mse.\n\nReturns True if the function performs correctly (computes mean squared error) and False otherwise.\n\nHint: we will test your function with three different functions: the correct one, one that returns the mean squared error of the keys (doesn’t use the counts), and one that always returns 42.\n\ntest_count_lengths(count_fnc=count_lengths):\n\nThis test function takes one input:\n\ncount_fnc: a function that takes one input parameter (a list of strings) and returns a dictionary. It has a default value of count_lengths.\n\nReturns True if the function performs correctly (creates a dictionary of counts of occurrences of lengths) and False otherwise.\n\nHint: we will test your function with three different functions: the correct one, one that returns the number of strings in the list, and one that always returns a dictionary with the key:value pair 42:42.\n\nTrying first on the correct function:\n\nprint(f'test_compute_mean(compute_mean) returns {test_compute_mean(compute_mean)}.')\n\n\n\ngives the output:\n\ntest_compute_mean(compute_mean) returns True.\n\nContinuing our example:\n\nprint(f'test_compute_mean( lambda x : 42 ) returns {test_compute_mean(lambda x : 42)}.')\n\n\n\ngives the output:\n\ntest_compute_mean( lambda x : 42 ) returns False.\n\nThe lambda in Python allows you to write small anonymous functions (see \n\nPython Docs 4.8.6 Lambda Expressions for more details).\n\n","type":"content","url":"/assignment1#testing-code","position":15},{"hierarchy":{"lvl1":"Assignment 1","lvl4":"Program Template","lvl3":"Program 1: School Counts","lvl2":"CSci 39542: Introduction to Data Science"},"type":"lvl4","url":"/assignment1#program-template","position":16},{"hierarchy":{"lvl1":"Assignment 1","lvl4":"Program Template","lvl3":"Program 1: School Counts","lvl2":"CSci 39542: Introduction to Data Science"},"content":"For this first program, we have included a template, \n\np1_template.py to use to get started. It includes function stubs and some testing in a (conditionally executed) main function:\n\n\"\"\"\n  Name: YOUR NAME HERE (as it appears in Gradescope)\n  Email: YOUR EMAIL HERE (as it appears in Gradescope)\n  Resources:  ANY RESOURCES YOU USED\n\"\"\"\nimport textwrap\n\ndef extract_overviews(file_name):\n  \"\"\"\n  Opens the file_name and from each line of the file, keeps the overview\n  description of the school (the fifth \"column\": overview_paragraph.\n  Returns a list of the paragraphs.\n  \"\"\"\n\n  #Placeholder-- replace with your code\n  lst = []\n  return lst\n\ndef count_lengths(overview_list):\n  \"\"\"\n  For each element of the overview_list, computes the length (# of characters).\n  Returns the dictionary of length occurrences.\n  \"\"\"\n\n  #Placeholder-- replace with your code\n  counts = {}\n  return counts\n\ndef count_sentences(overview_list):\n  \"\"\"\n  For each element of the overview_list, computes the number of periods \n  (as a proxy for the number of sentences).\n  Returns the dictionary of occurrences.\n  \"\"\"\n\n  #Placeholder-- replace with your code\n  counts = {}\n  return counts\n\n\ndef compute_mean(counts):\n  \"\"\"\n  Computes the mean of counts dictionary, weighting each key that occurs by its value.\n  Returns the mean. \n  \"\"\"\n\n  #Placeholder-- replace with your code\n  mean = 0\n  return mean\n\n\ndef compute_mse(theta, counts):\n  \"\"\"\n  Computes the Mean Squared Error of the parameter theta and a dictionary, counts.\n  Returns the MSE.\n  \"\"\"\n\n  #Placeholder-- replace with your code\n  mse = 0\n\n  return mse\n\ndef test_compute_mean(mean_fnc=compute_mean):\n  \"\"\"\n  Returns True if the mean_fnc performs correctly\n  (e.g. computes weighted mean of inputted dictionary) and False otherwise. \n  \"\"\"\n\n  #Placeholder-- replace with your code\n  correct = True\n  return correct\n\n\ndef test_mse(mse_fnc=compute_mse):\n  \"\"\"\n  Returns True if the extract_fnc performs correctly\n  (e.g. computes mean squared error) and False otherwise.\n  \"\"\"\n\n  #Placeholder-- replace with your code\n  correct = True\n  return correct\n\ndef test_count_lengths(counts_fnc=count_lengths):\n  \"\"\"\n  Returns True if the counts_fnc performs correctly\n  (e.g. counts lengths of overviews and stores in dictionary) & False otherwise.\n  \"\"\"\n\n  #Placeholder-- replace with your code\n\n  correct = True\n  return correct\n\n\ndef main():\n  \"\"\"\n  Some examples of the functions in use:\n  \"\"\"\n\n  ###Extracts the overviews from the data files:\n  file_name = 'fall23/program01/2021_DOE_High_School_Directory_SI.csv'\n  si_overviews = extract_overviews(file_name)\n  print(f\"Number of SI overviews: {len(si_overviews)}. The the last one is:\\n\")\n  #Using textwrap for prettier printing:\n  print(textwrap.fill(si_overviews[-1],80))\n\n  late_name = 'fall23/program01/2020_DOE_High_School_Directory_late_start.csv'\n  late_overviews = extract_overviews(late_name)\n  print(f\"\\n\\nNumber of late start overviews: {len(late_overviews)}. The the last one is:\\n\")\n  print(textwrap.fill(late_overviews[-1],80))\n\n  ###Computing counts and means:\n  si_len_counts = count_lengths(si_overviews)\n  print(f\"The {sum(si_len_counts.values())} entries have lengths:\")\n  print(si_len_counts)\n  late_len_counts = count_lengths(late_overviews)\n  print(f\"The {sum(late_len_counts.values())} entries have lengths:\")\n  print(late_len_counts)\n\n  si_dots_counts = count_sentences(si_overviews)\n  print(f\"The {sum(si_dots_counts.values())} entries have lengths:\")\n  print(si_dots_counts)\n  late_dots_counts = count_sentences(late_overviews)\n  print(f\"The {sum(late_dots_counts.values())} entries have lengths:\")\n  print(late_dots_counts)\n\n  si_len_mean = compute_mean(si_len_counts)\n  si_dots_mean = compute_mean(si_dots_counts)\n  print(f\"Staten Island high schools overviews had an average of {si_len_mean:.2f}\\\ncharacters in {si_dots_mean:.2f} sentences.\")\n\n  ###Computing MSE:\n  late_dots_mean = compute_mean(late_dots_counts)\n  print(f\"The mean for number of sentences in SI descriptions is {late_dots_mean}.\")\n  losses = []\n  for theta in range(10):\n      loss = compute_mse(theta,late_dots_counts)\n      print(f\"For theta = {theta}, MSE loss is {loss:.2f}.\")\n      losses.append(loss)\n\n  losses = []\n  for theta in range(10):\n      loss = compute_mse(theta,si_dots_counts)\n      print(f\"For theta = {theta}, MSE loss is {loss:.2f}.\")\n      losses.append(loss)\n\n  ###Testing\n  #Trying first on the correct function:\n  print(f'test_compute_mean(compute_mean) returns {test_compute_mean(compute_mean)}.')\n  #Trying on a function that returns 42 no matter what the output:\n  print(f'test_compute_mean( lambda x : 42 ) returns {test_compute_mean(lambda x : 42)}.')\n\nif __name__ == \"__main__\":\n  main()\n\n","type":"content","url":"/assignment1#program-template","position":17},{"hierarchy":{"lvl1":"Assignments"},"type":"lvl1","url":"/assignments","position":0},{"hierarchy":{"lvl1":"Assignments"},"content":"This page lists the assignment pages.\n\nAssignment 1 - [Assignment 2](assignment2)\n- [Assignment 3](assignment3)\n- [Assignment 4](assignment4)\n- [Assignment 5](assignment5)\n- [Assignment 6 (optional)](assignment6) ","type":"content","url":"/assignments","position":1},{"hierarchy":{"lvl1":"Schedule"},"type":"lvl1","url":"/home","position":0},{"hierarchy":{"lvl1":"Schedule"},"content":"","type":"content","url":"/home","position":1},{"hierarchy":{"lvl1":"Schedule","lvl2":"CSci 39542: Introduction to Data Science"},"type":"lvl2","url":"/home#csci-39542-introduction-to-data-science","position":2},{"hierarchy":{"lvl1":"Schedule","lvl2":"CSci 39542: Introduction to Data Science"},"content":"Department of Computer Science (\n\nHunter College, \n\nCity University of New York)Spring 2026\n\nTL;DR: data-focused programming course with optional project.","type":"content","url":"/home#csci-39542-introduction-to-data-science","position":3},{"hierarchy":{"lvl1":"Schedule","lvl3":"Calendar (Tentative schedule, subject to change)","lvl2":"CSci 39542: Introduction to Data Science"},"type":"lvl3","url":"/home#calendar-tentative-schedule-subject-to-change","position":4},{"hierarchy":{"lvl1":"Schedule","lvl3":"Calendar (Tentative schedule, subject to change)","lvl2":"CSci 39542: Introduction to Data Science"},"content":"Week\n\nDate\n\nTopics & Coverage\n\nReading\n\nWeek 0\n\nTuesday, 27 January\n\nSyllabus & Class Overview\n\n—\n\nWeek 1\n\nThursday, 29 January\n\nData Science Lifecycle; Data Scope; Big Data; Accuracy; Python Recap (dictionaries, I/O, keyword parameters, linting)\n\nDS 100: Chapter 1 (Data Science Lifecycle)  \n\nDS 100: Chapter 2 (Data Scope)  \n\nDS 100: Chapter 4 (Modeling with Summary Statistics)  \n\nThink CS: Chapter 12 (Dictionaries)  \n\nDS 100: Section 13.1 (String Methods)  \n\nThink CS: Chapter 11 (Files)  \n\npython.org: Section 4.7 (Functions)  \n\npylint documentation | Week 2 | Mon&Thurs, 8&11 September  | Expectations, Variance, Correlation, Residuals & Sampling; Linear Regression & Loss Functions; DataFrames; Python Recap (lambda & applying func.)         | [Seeing Theory (Brown U)](https://seeing-theory.brown.edu), [Guessing Correlation Coefficients (GeoGebra)](https://www.geogebra.org), [Computing Correlations (Real Python)](https://realpython.com), [Residuals (UBC)](https://shiney.zoology.ubc.ca), [DS 100: Chapter 3 (Simulation & Data Design)](https://learningds.org), [DS 100: Chapter 15 (Linear Models)](https://learningds.org), [DS 100: Chapter 6 (DataFrames)](https://learningds.org), [Constructing DataFrames (pydata.org)](https://pandas.pydata.org), [DS 100: Section 8.5 (Table Shape & Granularity)](https://learningds.org), [python.org: Section 4.7 (Functions)](https://docs.python.org/3/tutorial/) |\n|     | Wednesday, 17 September  | —                                                                                                                                                         | Program 1 Due| \n| Week 3 | Mon&Thurs, 15&18 September | Multiple Linear Regression; Missing Value Imputation; Feature Engineering; Joining & Transforming Data; Python Recap (list comprehensions, zip)           | [DS 100: Chapter 6 (DataFrames)](https://learningds.org)<br> [DS 100: Chapter 9 (Data Wrangling)](https://learningds.org)<br> [DS 100: Chapter 15 (Linear Models)](https://learningds.org)<br> [Think CS: Section 10.23 (List Comprehensions)](https://runestone.academy/ns/books/published/thinkcspy/index.html)<br> [Zip Tutorial (RealPython)](https://realpython.com/) |\n| Week 4 | Thursday, 25 September| Polynomial Models; Cross Validation; Regularization; Bias–Variance; | [DS 8: Chapter 15 (Prediction)](https://inferentialthinking.com)<br> [DS 100: Chapter 11 (Data Visualization)](https://learningds.org)<br> [DS 100: Section 15.4 (Multiple Linear Regression)](https://learningds.org)<br> [DS 100: Section 15.7 (Feature Engineering)](https://learningds.org)<br> [DS 100: Section 16.3 (Cross Validation)](https://learningds.org)<br> |\n| Week 5 | Monday, 29 September (Asynchronous)| sklearn modeling; Loss Functions; Plotly/matplotlib/seaborn visualizations; Time-series; Pickling; Python Recap (dates & times)                            | [DS 100: Sections 4.2–4.3 (Loss Functions)](https://learningds.org)<br> [DS 100: Chapter 10 (Exploratory Data Analysis)](https://learningds.org)<br> [DS 100: Sections 11.1–11.3 (Data Visualization)](https://learningds.org)<br> [DS 100: Chapter 15 (Linear Models)](https://learningds.org)<br> [Python Object Serialization Docs (Pickling)](https://docs.python.org/3/library/pickle.html)<br> [Hands On ML (Matplotlib Tools)](https://github.com/) |\n| Week 6 | Mon&Thurs, 6&9 October    | Testing, GIS Data Visualization (GeoJSON, Choropleth, Voronoi);Distributions, Gaussian PDFs, Smoothing; Hypothesis Testing; Central Limit Theorem;                                  |  [DS 100: Chapter 17 (Theory for Inference & Prediction)](https://learningds.org)<br> [Sampling from a Normally Distributed Population (UBC)](https://example.ubc.ca)<br> [Central Limit Theorem (UBC)](https://example.ubc.ca)<br> [Confidence Intervals (UBC)](https://example.ubc.ca) <br> [DS 100: Chapter 20 (Gradient Descent)](https://learningds.org)<br> [Folium documentation](https://python-visualization.github.io/folium/)<br> [GeoJSON Editor](https://geojson.io)<br> [Gradient Descent Visualization (Lili Jiang)](https://lili-j.com)<br> [ThinkCS: Unit Testing](https://runestone.academy/ns/books/published/thinkcspy/index.html)<br> [Pytest](https://docs.pytest.org/)|\n| | Wednesday, 8 October   | —                        | Program 2 Due| — |\n| Week 7 | Tues&Thurs, 14&16 October   | Logistic Regression; Midterm review; & Optional Project Discussion| [DS 100: Chapter 19 (Classification)](https://learningds.org)<br> [Recognizing Hand-Written Digits (sklearn)](https://scikit-learn.org)<br> [Confusion Matrices (sklearn)](https://scikit-learn.org)|                                  \n| Week 8 | Thurs&Fri, 23&24 October   | Midterm Exam (Thurs); Multiclass Classification; Naive Bayes; Decision Trees; Random Forests;| [DS 100: Chapter 19 (Classification)](https://learningds.org)<br> [DS 100: Chapter 22 (PCA)](https://learningds.org)<br> [Decision Trees & Bias–Variance (R2D3)](https://www.r2d3.us)<br>  |\n|       | Sunday, 26 October|-|                               Project Proposal due    | — |                                \n| Week 9 | Mon&Thurs, 27&30 October   |  Linear Algebra (vectors, matrices, eigenvalues); Classification (SVMs); Scree Plots; Special Topic of time series data and data quality                                                                     | [Eigenvectors/Eigenvalues visualization](https://example.edu)<br> [Linear Algebra Review (MIT)](https://ocw.mit.edu)<br> [Python DS Handbook Chapter 5 (SVMs)](https://jakevdp.github.io)<br> [Karpathy's SVM Demo (Stanford)](https://cs231n.github.io)<br> [DataCamp Tutorial (SVMs)](https://www.datacamp.com)<br> [SVMs (sklearn)](https://scikit-learn.org)      | \n|       | Tuesday, 28 October (No penalty late submission-Sunday, 2 November)  | -|Program 3 Due|  \n| Week 10| Mon&Thurs, 3&6 November   | Guest Speaker Talk : Data Science in Industry; Multidimensional Scaling (MDS); PCA Non-Euclidean Distances;                                                                                           |  [Python DS Handbook: PCA](https://jakevdp.github.io)<br> [PCA Explained Visually](https://example.edu); [Manifold Learning (Python DS Handbook)](https://jakevdp.github.io)<br> [Manifold Learning (sklearn)](https://scikit-learn.org) |\n| Week 11| Mon&Thurs, 10&13 November   | K-Means; Gaussian Mixture Models; Hierarchical & Spectral Clustering                                                                                     | [Spectral Clustering (Kaggle)](https://www.kaggle.com)<br> [Clustering (Carpentry)](https://carpentries.org)<br> [Spectral Clustering (Great Learning)](https://example.com)<br> [K-Means gif (wiki)](https://commons.wikimedia.org)<br> [Aurelien:Hands-on](../attachments/Aurélien%20Géron%20-%20Hands-On%20Machine%20Learning%20with%20Scikit-Learn%20Keras%20and%20TensorFlow%20-%20Concepts%20Tools%20and%20Techniques%20to%20Build%20Intelligent%20Systems%202nd%20Edition%202019.pdf)<br> [Python DS Handbook: K-Means](https://jakevdp.github.io)<br> [Handbook: GMM](https://jakevdp.github.io)<br> [Cluster Analysis (wiki)](https://en.wikipedia.org) |\n|Project Proposal Window Close| Friday, 14 November|-| — | — |\n| Week 12| Mon&Thurs, 17&20 November  | Supervised vs. Unsupervised Leart-SNE; UMAP; Regular Expressions; Relational Databases & SQL Basics                                                              |  [ML Overview (PDSH)](https://example.edu)<br> [Supervised vs Unsupervised (IBM)](https://www.ibm.com)<br> [DS 100: Sections 13.2–13.3 (Regular Expressions)](https://learningds.org) |\n|       | Wednesday, 19 November     | —                                                                                                                                                         | Program 4 Due| — |\n|-|Interim Check-In window Open: Monday, 17 November to Sunday, 23 November|-| Project Interim Check-In Due| — |                                 \n| Week 13| Monday, 24 November  | SQL: Aggregation, Joins, Transformations; More on Regex; Project Draft                                                                      |  [DS 100: Chapter 7 (Relational Databases & SQL)](https://learningds.org) |\n| Week 14      | 27–28 November          | Thanksgiving Break: No Classes                                                                                                                             | —                                | \n| Week 15| Mon&Thurs, 1&4 December   | Database & Research                                                                                                                    |  [DS 100: Chapter 7 (Relational Databases & SQL)](https://learningds.org) |\n|-| Wednesday, 3 December | -| Program 5 Due|\n|Week 16 | Mon&Thurs, 8&11 December | Demo & Semester Review||\n|Week 17 | Monday, 15 December | Review Session and Final Code Exam| TBD|\n|-| (Wednesday, 10 December) | -| (Optional) Program 6 Due|\n| Final Week    | 18 December 11:30 to 13:30   |Final Written Exam                                                                        | North 1001C                                | — |\n ","type":"content","url":"/home#calendar-tentative-schedule-subject-to-change","position":5},{"hierarchy":{"lvl1":"Syllabus"},"type":"lvl1","url":"/syllabus","position":0},{"hierarchy":{"lvl1":"Syllabus"},"content":"","type":"content","url":"/syllabus","position":1},{"hierarchy":{"lvl1":"Syllabus","lvl2":"CSci 39542: Introduction to Data Science"},"type":"lvl2","url":"/syllabus#csci-39542-introduction-to-data-science","position":2},{"hierarchy":{"lvl1":"Syllabus","lvl2":"CSci 39542: Introduction to Data Science"},"content":"Department of Computer Science, Hunter College, City University of New YorkSpring 2026","type":"content","url":"/syllabus#csci-39542-introduction-to-data-science","position":3},{"hierarchy":{"lvl1":"Syllabus","lvl2":"Description"},"type":"lvl2","url":"/syllabus#description","position":4},{"hierarchy":{"lvl1":"Syllabus","lvl2":"Description"},"content":"3 hours, 3 credits: This topics course focuses on computational methods and statistical techniques to analyze data and make inferences. Topics include data collection and cleaning, exploratory data analysis and visualization, and statistical inference and prediction. Students will acquire a working knowledge of data science through hands-on projects with real-world data. Basic proficiency in statistics and Python programming is assumed, as well as experience with abstract data structures.\n\nPrerequisites: CSci 127, Stat 213, and one of: CSci 133 or CSci 235.\n\nInstructor: \n\nRyan Vaz Office hour: 4.00PM - 5.00PM TuTh North 1008 or by appointment","type":"content","url":"/syllabus#description","position":5},{"hierarchy":{"lvl1":"Syllabus","lvl2":"Grading Policy"},"type":"lvl2","url":"/syllabus#grading-policy","position":6},{"hierarchy":{"lvl1":"Syllabus","lvl2":"Grading Policy"},"content":"Course Format:\n\nSynchronous in-person lectures are each Tuesdays and Thursday, 5:30pm-6:45pm at North 1001E - Individual programming assignments and (optional) cumulative project \n\nMidterm examination during lecture and final examination during the registrar-assigned time slot during finals week\n\nExpectations:\n\nCompleting homework is an essential part of the learning experience. Students are expected to learn both the material covered in class and the material in the textbook and other assigned reading.\n\nHonor Code:\n\nYou are encouraged to work together on the overall design of the programs and homework. However, for specific programs and homework assignments, all work must be your own. As a general rule, do your own typing. Submitting work of others, or not safeguarding your work from copying, are academic integrity violations. You are responsible for knowing and following Hunter College’s \n\nAcademic Integrity Policy:\n\nHunter College regards acts of academic dishonesty (e.g., plagiarism, cheating on examinations, obtaining unfair advantage, and falsification of records and official documents) as serious offenses against the values of intellectual honesty. The College is committed to enforcing the CUNY Policy on Academic Integrity and will pursue cases of academic dishonesty according to the Hunter College Academic Integrity Procedures. All incidents of cheating will be reported to the Office of Student Conduct in the Vice President for Student Affairs and Dean of Students office.\n\nLecture Participation:\n\nParticipation in lecture is measured by collected classwork. If you miss or do poorly on a classwork, your grade on the final exam will replace the missing or low grade.\n\nProgramming Assignments:\n\nAssignments are posted on the class website, usually two weeks before the due date. They reinforce concepts covered in lecture and lab and serve as building blocks for the classwork and the semester-long project.\n\nTo receive full credit for a program, the program must perform correctly, must include comments, be written in good style (following \n\nPEP 8, using \n\nPylint), and be submitted via \n\nGradescope.\n\nWhile you may consult and discuss with others, this is an individual assignment, and all code must be written and typed by you. All programs are run through \n\nsimilarity review and copied code is reported to the Office of Student Conduct. - To encourage starting early on assignments, bonus points are available for submitting the programming assignments early ([more details](https://stjohn.github.io/datasci/fall23/work.html#hw)). \n\nNo late homework is accepted. Instead, we drop the lowest programming assignment grade when computing the final programming grade.\n\nMidterm Examination: **Project:** There is a 2-part exam consisting of written and coding questions:  There is a 2-part exam consisting of written and coding questions: - Thursday, 23 October: 9:00am-11:15am \n\nThe midterm covers material from the lecture notes, code demonstrations, classwork, and submitted programs. There is no make-up midterm examination. Instead, your score on the final exam will replace missing midterm grades (the final exam grade will also replace the midterm grade if you earn a higher grade on the final than the midterm).\n\nFinal Exam:\nThere is a 2-part exam consisting of written and coding questions:\n\nCoding exam will be given either during finals week or the last week of class. It is a case-study style exam where you will be given a dataset and asked to perform various data science tasks on it within 24 hours. Expectation and rubric will be provided closer to the exam date.\n\nWriting exam: more details to come ... will be given during finals week on the day and time assigned by the registrar.The written final exam is required and is comprehensive, covering all the material of the course. Sample exam questions will be available during the last weeks of the term. \n\nProject:\nA final project is optional for this course. The grade for the project is a combination of grades earned on the milestones (e.g. deadlines during the semester to keep the projects on track) and the overall submitted program. If you choose not to complete the project, your final exam grade will replace its portion of the overall grade. More details can be found on the \n\nproject page.\n\nGrades:\n\nParticipation: 10%\n\nProgramming Assignments: 20%\n\nOptional Project: 25% (if you choose not to do the project, the final exam replaces the grade)\n\nExams:\n\nMidterm Exam: 20%\n\nFinal Exam: 25% (or 50% if you choose not to do the optional project) - Extra Credit (optional):\n  - Up to 5% extra credit can be earned via [pandas 30 days of code challenge](https://leetcode.com/problemset/pandas/)\n  - 32 total exercises with premium membership (locked symbol), but for regular free membership, there are 27 exercises available. The total extra credit will be earned by the 27 exercises not 32 questions. \n\nEmergencies:\n\nDrop the lowest programming grade\n\nReplace low or missing grades on the midterm, classwork participation and project with your final exam grade\n\nTo respect your privacy, there is no need to provide documentation to take advantage of the dropping/replacing grades policies. It is done automatically. See individual sections above for details. If you are going to miss more than 2 weeks of class and associated work, contact us, so we can make arrangements for you to take the course in a future term.","type":"content","url":"/syllabus#grading-policy","position":7},{"hierarchy":{"lvl1":"Syllabus","lvl2":"Materials, Resources and Accommodating Disabilities"},"type":"lvl2","url":"/syllabus#materials-resources-and-accommodating-disabilities","position":8},{"hierarchy":{"lvl1":"Syllabus","lvl2":"Materials, Resources and Accommodating Disabilities"},"content":"This is a zero cost course. All textbook materials are freely available to enrolled students.\n\nTextbook & Readings:\n\nUC Berkeley’s Data Science 8 (DS 8): \n\nThe Foundations of Data Science by Ani Adhikari and John DeNero.\n\nUC Berkeley’s Data Science 100 (DS 100): \n\nPrinciples and Techniques of Data Science by Sam Lau, Joey Gonzalez, and Deb Nolan.\n\nAdditional readings and tutorials are available on the \n\nresources page.\n\nTechnology:\n\nThis is a programming-intensive course in the Python programming language. See the resource page on Brightspace for obtaining Python and the packages used, links for submitting assignments and assessments. All software used is freely available.\n\nComputer Access:\n\nA computer (capable of running Python 3) is needed to complete the on-line assessments, and programming assignments and projects. Hunter College is committed to all students having the technology needed for their courses. If you are in need of technology, see \n\nStudent Life’s Support & Resources Page.\n\nAccommodating Disabilities:\n\nIn compliance with the American Disability Act of 1990 (ADA) and with Section 504 of the Rehabilitation Act of 1973, Hunter College is committed to ensuring educational parity and accommodations for all students with documented disabilities and/or medical conditions. It is recommended that all students with documented disabilities (Emotional, Medical, Physical, and/or Learning) consult the \n\nOffice of AccessABILITY. For further information and assistance, see their \n\ncontact page.\n\nHunter College Policy on Sexual Misconduct:\n\nIn compliance with the CUNY Policy on Sexual Misconduct, Hunter College reaffirms the prohibition of any sexual misconduct, which includes sexual violence, sexual harassment, and gender-based harassment retaliation against students, employees, or visitors, as well as certain intimate relationships. Students who have experienced any form of sexual violence on or off campus (including CUNY-sponsored trips and events) are entitled to the rights outlined in the Bill of Rights for Hunter College.\n\nSexual Violence: Students are strongly encouraged to immediately report the incident by calling 911, contacting NYPD Special Victims Division Hotline (646-610-7272) or their local police precinct, or contacting the College’s Public Safety Office (212-772-4444).\n\nAll Other Forms of Sexual Misconduct: Students are also encouraged to contact the College’s Title IX Campus Coordinator, Dean John Rose (\n\njtrose@hunter​.cuny​.edu or 212-650-3262) or Colleen Barry (\n\ncolleen​.barry@hunter​.cuny​.edu or 212-772-4534) and seek complimentary services through the Counseling and Wellness Services Office, Hunter East 1123.\n\nSee \n\nCUNY Policy on Sexual Misconduct Link.","type":"content","url":"/syllabus#materials-resources-and-accommodating-disabilities","position":9},{"hierarchy":{"lvl1":"Syllabus","lvl2":"Course Objectives"},"type":"lvl2","url":"/syllabus#course-objectives","position":10},{"hierarchy":{"lvl1":"Syllabus","lvl2":"Course Objectives"},"content":"At the end of the course, students should be able to:\n\nAcquire data sets from multiple sources and write programs that can extract, transform, and load the data into a usable form.\n\nUse exploratory data analysis and visualization techniques as well as linear algebra and statistical inference to extract new insights from the data.\n\nApply predictive modeling and machine learning techniques to medium and large datasets.\n\nUnderstand the theory and interpret the results of predictive models and machine learning models. [CSci 39542 Home](https://stjohn.github.io/datasci/fall23/index.html) | [Resources](https://stjohn.github.io/datasci/fall23/resources.html) | [Coursework](https://stjohn.github.io/datasci/fall23/work.html) | [FAQ](https://stjohn.github.io/datasci/fall23/faq.html) ","type":"content","url":"/syllabus#course-objectives","position":11},{"hierarchy":{"lvl1":"Syllabus","lvl2":"Important Notes:"},"type":"lvl2","url":"/syllabus#important-notes","position":12},{"hierarchy":{"lvl1":"Syllabus","lvl2":"Important Notes:"},"content":"The syllabus is subject to change; more details will be provided as they become available.","type":"content","url":"/syllabus#important-notes","position":13}]}