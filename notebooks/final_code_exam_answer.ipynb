{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91d5476",
   "metadata": {},
   "source": [
    "# Text Classification \n",
    "The objective of this case study is to explore data handling techniques. \n",
    "\n",
    "**Data Source**: The [Spambase Dataset](../attachments/spambase.csv) is a collection of email messages labeled as spam or not spam.\n",
    "\n",
    "## Instruction:\n",
    "1. First read the entire case study description before starting to code; make notes down the control flow, expected functionality of the various methods and why you are implementing them <br>\n",
    "2. You are allowed to use scikit-learn libriries for building models and libraries for text preprocessing. **If you use any other tools and IDEs, please mention them in the report.** If you do not declare the use of AI tools, it will be considered as academic dishonesty.<br>\n",
    "3. Make sure to answer all questions listed including these sub-questions in your report to get full credits. And your code should be well-commented to explain your logic.<br>\n",
    "4. The following deliverables should be compressed and submitted as a single zip file as '<lastname>_<firstname>_final_case_study.zip' bia **Brightspace**.\n",
    "\t**Deliverables**:\n",
    "\t1. Source code (Jupyter Notebook or Python scripts) implementing the tasks outlined below.\n",
    "\t2. A brief report (Markdown or PDF) summarizing your findings and methodologies in two pages (approximately 1000 words). The report should include:\n",
    "\t3. Outputs from each part of the case study (you can copy-paste relevant plots and tables from your code outputs).\n",
    "\t4. Which AI/tools assists you used (e.g., GitHub Copilot, ChatGPT, etc.)\n",
    "\t5. Limitations and challenges you faced while using AI\n",
    "\t\t1. Do the tools make mistakes? If so, what kind?\n",
    "\t\t2. How did you verify the correctness of the AI-generated code or suggestions?\n",
    "\t6. Ethical considerations when using AI for data science tasks\n",
    "\t7. References to any external resources or documentation you consulted.\n",
    "\n",
    "If you have any questions, please contact me via email directly. I can answer via zoom calls as well. Please do it by your own efforts and do not share your code with others. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90546d3b",
   "metadata": {},
   "source": [
    "## Tasks\n",
    "Questions you need to answer in this case study are organized into 4 parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5347e7a",
   "metadata": {},
   "source": [
    "### Part 1 — Data Understanding (15 pts)\n",
    "1. Import Libraries and Load Data (5 pts)\n",
    "2. Dataset description (size, features, target meaning, and etc.,) (5 pts)\n",
    "\t - look up top 5 rows \n",
    "\t - get dimension of data \n",
    "\t - get class distribution \n",
    "\t - generate a bar plot to display the class distribution\n",
    "3. Create a separate feature set (data matrix X) and Target (1D vector y) and print dimension of each (5 pts)\n",
    "   - create train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b623db73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the spambase dataset\n",
    "# df = pd.read_csv('../attachments/spambase.csv')\n",
    "\n",
    "# # Check for missing values\n",
    "# print(\"Missing values per column:\")\n",
    "# print(df.isnull().sum())\n",
    "# print(\"\\nTotal missing values:\", df.isnull().sum().sum())\n",
    "# print(\"\\nPercentage of missing values:\")\n",
    "# print((df.isnull().sum() / len(df) * 100).round(2))\n",
    "# print(\"\\nDataset shape:\", df.shape)\n",
    "# print(\"\\nFirst few rows:\")\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc0f0d",
   "metadata": {},
   "source": [
    "<!-- ## Part 1 — Data Understanding (15 pts)\n",
    "1. Describe the EEG Eye State dataset.\n",
    "\t- How many instances and features are there?\n",
    "\t- What does the target variable represent?\n",
    "2. Inspect the data quality.\n",
    "\t- What percentage of values are missing per feature?\n",
    "\t- Is missingness distributed uniformly or concentrated in certain sensors?\n",
    "3. Plot the distribution of at least 2 randomly chosen EEG channels.\n",
    "\t- What patterns do you notice? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287499eb",
   "metadata": {},
   "source": [
    "### Part 2 — Data Handling (35 pts)\n",
    "1. Data Quality & Outlier Detection (10 pts)\n",
    "   - Identify and visualize outliers using statistical methods \n",
    "   - Discuss impact of outliers on classification performance and decide whether to keep, remove, or cap outliers and justify your choice\n",
    "2. Feature Scaling/Normalization (10 pts)\n",
    "   - Apply standardization (z-score) and/or normalization (min-max)\n",
    "   - Justify why scaling is important for spam classification\n",
    "3. Handling Class Imbalance (5 pts)\n",
    "   - Apply techniques if imbalanced (oversampling, undersampling, or class weights)\n",
    "4. Feature Importance Analysis (5 pts)\n",
    "   - Identify which word/character features are most predictive of spam\n",
    "   <!-- - Use correlation analysis, mutual information, or permutation importance -->\n",
    "   - Visualize top 10-15 important features\n",
    "5. Dimensionality Reduction/Visualization (5 pts)\n",
    "   - Apply PCA or t-SNE to reduce features for visualization\n",
    "   <!-- - Plot spam vs. non-spam clusters in 2D space -->\n",
    "   - Discuss separability and implications for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73ef2a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.manifold import TSNE\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# \n",
    "# # Load the spambase dataset\n",
    "# df = pd.read_csv('../attachments/spambase.csv')\n",
    "# \n",
    "# # Separate features (X) and target (y)\n",
    "# # Assuming last column is the target (spam: 1, not spam: 0)\n",
    "# X = df.iloc[:, :-1]\n",
    "# y = df.iloc[:, -1]\n",
    "# \n",
    "# print(\"Dataset shape:\", X.shape)\n",
    "# print(\"Target distribution:\")\n",
    "# print(y.value_counts())\n",
    "# print(\"\\nFeature statistics (first 5 features):\")\n",
    "# print(X.iloc[:, :5].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935cfc29",
   "metadata": {},
   "source": [
    "## 2.1 Data Quality & Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e19a67af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Outlier detection using IQR method\n",
    "# def detect_outliers_iqr(data, column):\n",
    "#     Q1 = data[column].quantile(0.25)\n",
    "#     Q3 = data[column].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "#     lower_bound = Q1 - 1.5 * IQR\n",
    "#     upper_bound = Q3 + 1.5 * IQR\n",
    "#     return (data[column] < lower_bound) | (data[column] > upper_bound)\n",
    "# \n",
    "# # Detect outliers in each feature\n",
    "# outlier_counts = {}\n",
    "# for col in X.columns:\n",
    "#     outliers = detect_outliers_iqr(X, col).sum()\n",
    "#     outlier_counts[col] = outliers\n",
    "# \n",
    "# print(\"Outlier counts per feature (IQR method):\")\n",
    "# print(f\"Features with outliers: {sum(1 for v in outlier_counts.values() if v > 0)}\")\n",
    "# print(f\"Total outlier instances: {sum(outlier_counts.values())}\")\n",
    "# \n",
    "# # Visualize outliers in top features\n",
    "# fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "# axes = axes.ravel()\n",
    "# top_features = sorted(outlier_counts.items(), key=lambda x: x[1], reverse=True)[:4]\n",
    "# \n",
    "# for idx, (feature, count) in enumerate(top_features):\n",
    "#     axes[idx].boxplot([X[feature][y == 0], X[feature][y == 1]], labels=['Non-Spam', 'Spam'])\n",
    "#     axes[idx].set_title(f'{feature}\\n(Outliers: {count})')\n",
    "#     axes[idx].set_ylabel('Value')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.savefig('outlier_detection.png', dpi=100, bbox_inches='tight')\n",
    "# plt.show()\n",
    "# \n",
    "# print(\"\\nDecision: Keep outliers (they may represent legitimate spam characteristics like excessive punctuation)\")\n",
    "# print(\"Outliers will be retained as they are informative for spam detection.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b9ca6",
   "metadata": {},
   "source": [
    "## 2.2 Feature Scaling/Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef94a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Compare feature scaling approaches\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "# \n",
    "# # Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "# \n",
    "# # 1. No scaling (baseline)\n",
    "# lr_no_scale = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# lr_no_scale.fit(X_train, y_train)\n",
    "# y_pred_no_scale = lr_no_scale.predict(X_test)\n",
    "# y_pred_proba_no_scale = lr_no_scale.predict_proba(X_test)[:, 1]\n",
    "# \n",
    "# acc_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
    "# f1_no_scale = f1_score(y_test, y_pred_no_scale)\n",
    "# auc_no_scale = roc_auc_score(y_test, y_pred_proba_no_scale)\n",
    "# \n",
    "# print(\"Performance WITHOUT scaling:\")\n",
    "# print(f\"  Accuracy: {acc_no_scale:.4f}, F1-Score: {f1_no_scale:.4f}, AUC: {auc_no_scale:.4f}\\n\")\n",
    "# \n",
    "# # 2. Standardization (Z-score)\n",
    "# scaler_std = StandardScaler()\n",
    "# X_train_std = scaler_std.fit_transform(X_train)\n",
    "# X_test_std = scaler_std.transform(X_test)\n",
    "# \n",
    "# lr_std = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# lr_std.fit(X_train_std, y_train)\n",
    "# y_pred_std = lr_std.predict(X_test_std)\n",
    "# y_pred_proba_std = lr_std.predict_proba(X_test_std)[:, 1]\n",
    "# \n",
    "# acc_std = accuracy_score(y_test, y_pred_std)\n",
    "# f1_std = f1_score(y_test, y_pred_std)\n",
    "# auc_std = roc_auc_score(y_test, y_pred_proba_std)\n",
    "# \n",
    "# print(\"Performance WITH Standardization (Z-score):\")\n",
    "# print(f\"  Accuracy: {acc_std:.4f}, F1-Score: {f1_std:.4f}, AUC: {auc_std:.4f}\\n\")\n",
    "# \n",
    "# # 3. Min-Max Normalization\n",
    "# scaler_minmax = MinMaxScaler()\n",
    "# X_train_minmax = scaler_minmax.fit_transform(X_train)\n",
    "# X_test_minmax = scaler_minmax.transform(X_test)\n",
    "# \n",
    "# lr_minmax = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# lr_minmax.fit(X_train_minmax, y_train)\n",
    "# y_pred_minmax = lr_minmax.predict(X_test_minmax)\n",
    "# y_pred_proba_minmax = lr_minmax.predict_proba(X_test_minmax)[:, 1]\n",
    "# \n",
    "# acc_minmax = accuracy_score(y_test, y_pred_minmax)\n",
    "# f1_minmax = f1_score(y_test, y_pred_minmax)\n",
    "# auc_minmax = roc_auc_score(y_test, y_pred_proba_minmax)\n",
    "# \n",
    "# print(\"Performance WITH Min-Max Normalization:\")\n",
    "# print(f\"  Accuracy: {acc_minmax:.4f}, F1-Score: {f1_minmax:.4f}, AUC: {auc_minmax:.4f}\\n\")\n",
    "# \n",
    "# # Comparison table\n",
    "# scaling_comparison = pd.DataFrame({\n",
    "#     'Scaling Method': ['No Scaling', 'Standardization', 'Min-Max Normalization'],\n",
    "#     'Accuracy': [acc_no_scale, acc_std, acc_minmax],\n",
    "#     'F1-Score': [f1_no_scale, f1_std, f1_minmax],\n",
    "#     'AUC': [auc_no_scale, auc_std, auc_minmax]\n",
    "# })\n",
    "# \n",
    "# print(\"Scaling Comparison:\")\n",
    "# print(scaling_comparison)\n",
    "# \n",
    "# # Visualization\n",
    "# fig, ax = plt.subplots(figsize=(10, 5))\n",
    "# scaling_comparison.set_index('Scaling Method')[['Accuracy', 'F1-Score', 'AUC']].plot(kind='bar', ax=ax)\n",
    "# plt.title('Impact of Feature Scaling on Logistic Regression Performance')\n",
    "# plt.ylabel('Score')\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.legend(loc='lower right')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('scaling_comparison.png', dpi=100, bbox_inches='tight')\n",
    "# plt.show()\n",
    "# \n",
    "# print(\"\\nConclusion: Standardization performs best; selected for further analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe20064",
   "metadata": {},
   "source": [
    "## 2.3 Handling Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7849252b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "# \n",
    "# # Analyze class distribution\n",
    "# print(\"Class Distribution (Original Data):\")\n",
    "# print(y.value_counts())\n",
    "# print(f\"\\nClass ratio (Spam:Non-Spam) = {y.sum()} : {len(y) - y.sum()}\")\n",
    "# print(f\"Imbalance ratio: {(len(y) - y.sum()) / y.sum():.2f}:1\\n\")\n",
    "# \n",
    "# # Visualize\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "# \n",
    "# # Original distribution\n",
    "# y.value_counts().plot(kind='bar', ax=axes[0], color=['steelblue', 'orange'])\n",
    "# axes[0].set_title('Original Class Distribution')\n",
    "# axes[0].set_ylabel('Count')\n",
    "# axes[0].set_xticklabels(['Non-Spam (0)', 'Spam (1)'], rotation=0)\n",
    "# \n",
    "# # Pie chart\n",
    "# axes[1].pie(y.value_counts(), labels=['Non-Spam', 'Spam'], autopct='%1.1f%%', colors=['steelblue', 'orange'])\n",
    "# axes[1].set_title('Proportion of Spam vs. Non-Spam')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.savefig('class_distribution.png', dpi=100, bbox_inches='tight')\n",
    "# plt.show()\n",
    "# \n",
    "# # Apply oversampling to balance classes\n",
    "# ros = RandomOverSampler(random_state=42)\n",
    "# X_resampled, y_resampled = ros.fit_resample(X_train_std, y_train)\n",
    "# \n",
    "# print(\"Class Distribution (After Oversampling):\")\n",
    "# print(pd.Series(y_resampled).value_counts())\n",
    "# print(f\"New dataset size: {len(y_resampled)} (was {len(y_train)})\")\n",
    "# \n",
    "# # Train model with balanced data\n",
    "# lr_balanced = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# lr_balanced.fit(X_resampled, y_resampled)\n",
    "# y_pred_balanced = lr_balanced.predict(X_test_std)\n",
    "# y_pred_proba_balanced = lr_balanced.predict_proba(X_test_std)[:, 1]\n",
    "# \n",
    "# # Evaluate with multiple metrics\n",
    "# from sklearn.metrics import precision_score, recall_score\n",
    "# \n",
    "# print(\"\\nPerformance Comparison (on test set):\")\n",
    "# comparison_df = pd.DataFrame({\n",
    "#     'Model': ['Without Balancing', 'With Oversampling'],\n",
    "#     'Accuracy': [\n",
    "#         accuracy_score(y_test, y_pred_std),\n",
    "#         accuracy_score(y_test, y_pred_balanced)\n",
    "#     ],\n",
    "#     'Precision': [\n",
    "#         precision_score(y_test, y_pred_std),\n",
    "#         precision_score(y_test, y_pred_balanced)\n",
    "#     ],\n",
    "#     'Recall': [\n",
    "#         recall_score(y_test, y_pred_std),\n",
    "#         recall_score(y_test, y_pred_balanced)\n",
    "#     ],\n",
    "#     'F1-Score': [\n",
    "#         f1_score(y_test, y_pred_std),\n",
    "#         f1_score(y_test, y_pred_balanced)\n",
    "#     ]\n",
    "# })\n",
    "# \n",
    "# print(comparison_df)\n",
    "# \n",
    "# print(\"\\nConclusion: Oversampling improves recall (catches more spam) at slight cost of precision.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a8cef2",
   "metadata": {},
   "source": [
    "## 2.4 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caec22af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature importance using Random Forest\n",
    "# rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "# rf_model.fit(X_train_std, y_train)\n",
    "# \n",
    "# # Get feature importances\n",
    "# feature_importance = pd.DataFrame({\n",
    "#     'Feature': X.columns,\n",
    "#     'Importance': rf_model.feature_importances_\n",
    "# }).sort_values('Importance', ascending=False)\n",
    "# \n",
    "# print(\"Top 15 Most Important Features:\")\n",
    "# print(feature_importance.head(15))\n",
    "# \n",
    "# # Visualization\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "# \n",
    "# # Top 15 features bar plot\n",
    "# top_15 = feature_importance.head(15)\n",
    "# axes[0].barh(range(len(top_15)), top_15['Importance'])\n",
    "# axes[0].set_yticks(range(len(top_15)))\n",
    "# axes[0].set_yticklabels(top_15['Feature'])\n",
    "# axes[0].invert_yaxis()\n",
    "# axes[0].set_xlabel('Importance Score')\n",
    "# axes[0].set_title('Top 15 Most Important Features for Spam Detection')\n",
    "# \n",
    "# # Cumulative importance\n",
    "# cumsum = feature_importance['Importance'].cumsum() / feature_importance['Importance'].sum()\n",
    "# axes[1].plot(range(len(cumsum)), cumsum, marker='o', linestyle='-', linewidth=2)\n",
    "# axes[1].axhline(y=0.8, color='r', linestyle='--', label='80% threshold')\n",
    "# axes[1].axhline(y=0.9, color='orange', linestyle='--', label='90% threshold')\n",
    "# axes[1].set_xlabel('Number of Features')\n",
    "# axes[1].set_ylabel('Cumulative Importance')\n",
    "# axes[1].set_title('Cumulative Feature Importance')\n",
    "# axes[1].legend()\n",
    "# axes[1].grid(True, alpha=0.3)\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.savefig('feature_importance.png', dpi=100, bbox_inches='tight')\n",
    "# plt.show()\n",
    "# \n",
    "# # Calculate how many features needed for 80% and 90% importance\n",
    "# n_features_80 = (cumsum >= 0.8).argmax() + 1\n",
    "# n_features_90 = (cumsum >= 0.9).argmax() + 1\n",
    "# \n",
    "# print(f\"\\nFeatures needed for 80% importance: {n_features_80}\")\n",
    "# print(f\"Features needed for 90% importance: {n_features_90}\")\n",
    "# print(f\"Total features: {len(X.columns)}\")\n",
    "# \n",
    "# print(\"\\nInterpretation: The most important features are likely word/character frequencies\")\n",
    "# print(\"that are characteristic of spam emails (e.g., '$', 'free', '!', etc.).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c7e87c",
   "metadata": {},
   "source": [
    "## 2.5 Dimensionality Reduction & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3a68b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA for dimensionality reduction\n",
    "# pca = PCA()\n",
    "# X_pca = pca.fit_transform(X_train_std)\n",
    "# \n",
    "# # Calculate cumulative variance explained\n",
    "# cumsum_var = np.cumsum(pca.explained_variance_ratio_)\n",
    "# \n",
    "# fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "# \n",
    "# # Scree plot\n",
    "# axes[0].plot(range(1, min(51, len(cumsum_var)+1)), cumsum_var[:50], marker='o', linestyle='-', linewidth=2)\n",
    "# axes[0].axhline(y=0.95, color='r', linestyle='--', label='95% variance')\n",
    "# axes[0].set_xlabel('Number of Components')\n",
    "# axes[0].set_ylabel('Cumulative Variance Explained')\n",
    "# axes[0].set_title('PCA: Cumulative Variance Explained')\n",
    "# axes[0].legend()\n",
    "# axes[0].grid(True, alpha=0.3)\n",
    "# \n",
    "# # PCA 2D visualization\n",
    "# pca_2d = PCA(n_components=2)\n",
    "# X_pca_2d = pca_2d.fit_transform(X_train_std)\n",
    "# \n",
    "# scatter = axes[1].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y_train, cmap='RdYlBu', alpha=0.6, s=30)\n",
    "# axes[1].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)')\n",
    "# axes[1].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)')\n",
    "# axes[1].set_title('PCA: 2D Visualization of Spam vs Non-Spam')\n",
    "# cbar = plt.colorbar(scatter, ax=axes[1])\n",
    "# cbar.set_label('Class (0=Non-Spam, 1=Spam)')\n",
    "# \n",
    "# plt.tight_layout()\n",
    "# plt.savefig('pca_analysis.png', dpi=100, bbox_inches='tight')\n",
    "# plt.show()\n",
    "# \n",
    "# n_components_95 = (cumsum_var >= 0.95).argmax() + 1\n",
    "# print(f\"\\nPCA Analysis:\")\n",
    "# print(f\"Components needed for 95% variance: {n_components_95} out of {X_train_std.shape[1]}\")\n",
    "# print(f\"Variance explained by first 2 components: {cumsum_var[1]:.2%}\")\n",
    "# \n",
    "# # t-SNE visualization (optional, slower)\n",
    "# print(\"\\nApplying t-SNE (this may take a moment)...\")\n",
    "# tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)\n",
    "# X_tsne = tsne.fit_transform(X_train_std[:5000])  # Use subset for speed\n",
    "# \n",
    "# fig, ax = plt.subplots(figsize=(10, 8))\n",
    "# scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_train[:5000], cmap='RdYlBu', alpha=0.6, s=30)\n",
    "# ax.set_xlabel('t-SNE Dimension 1')\n",
    "# ax.set_ylabel('t-SNE Dimension 2')\n",
    "# ax.set_title('t-SNE: 2D Visualization of Spam vs Non-Spam')\n",
    "# cbar = plt.colorbar(scatter, ax=ax)\n",
    "# cbar.set_label('Class (0=Non-Spam, 1=Spam)')\n",
    "# plt.tight_layout()\n",
    "# plt.savefig('tsne_analysis.png', dpi=100, bbox_inches='tight')\n",
    "# plt.show()\n",
    "# \n",
    "# print(\"\\nObservations:\")\n",
    "# print(\"- PCA shows moderate separation between spam and non-spam emails\")\n",
    "# print(\"- t-SNE reveals better clustering, suggesting the classes are somewhat separable\")\n",
    "# print(\"- High-dimensional feature space contains discriminative information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9236a388",
   "metadata": {},
   "source": [
    "## Part 3 — Modeling and Classification (35 pts)\n",
    "1. Baseline classifier performance comparison (drop vs impute) 10 pts\n",
    "\t- Metrics: accuracy, F1-score, AUC\n",
    "    - Compare table/plot\n",
    "2. Evaluate impact on metrics (accuracy, precision, recall, F1-score)\n",
    "    - Compare model performance with and without scaling\n",
    "    - Compare model performance with and without handling class imbalance\n",
    "3. Advanced model training and performance discussion 15 pts\n",
    "\t- Model choice justification\n",
    "\t- Performance comparison with baseline\n",
    "(Extra Credit) Can you beat the training accuracy of 97% and testing accuracy of 94%. Is your approach generalizable (bias-variance tradeoff)? Explain your approach and discuss. \n",
    "<!-- 6.\tTrain a baseline classifier (Logistic Regression or Random Forest) on:\n",
    "A. Dataset with missing values removed (drop rows)\n",
    "B. Dataset after your best imputation\n",
    "\t- Compare accuracy, F1-score, and AUC.\n",
    "7.\tExplain how temporal structure matters in this dataset.\n",
    "  \t- Although not explicitly time-tagged, why is EEG inherently temporal?\n",
    "\t- What limitations does this introduce for your model?\n",
    "8.\tTrain a more advanced model such as:\n",
    "\t- Gradient Boosted Trees\n",
    "\t- 1D-CNN\n",
    "\t- LSTM (if you reconstruct small sequences)\n",
    "\n",
    "Discuss whether it improves performance. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b10850a",
   "metadata": {},
   "source": [
    "<!-- ## Part 4 — Feature Importance & Interpretation\n",
    "9.\tCompute feature importance using either:\n",
    "\t- Permutation importance\n",
    "\t- SHAP values\n",
    "\n",
    "Which EEG channels contribute most to predicting eye state?\n",
    "\n",
    "10.\tDiscuss whether missing data in those important channels caused major performance loss. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9e9ca",
   "metadata": {},
   "source": [
    "<!-- ## Part 5 — Robustness & Sensitivity Analysis\n",
    "11.\tSimulate more missing data (20%, 30%, MCAR vs MAR).\n",
    "\t- How does your classifier degrade?\n",
    "\t- Plot performance vs. missing rate.\n",
    "12.\tTest robustness by intentionally corrupting a critical EEG channel.\n",
    "\t- How much accuracy drops\n",
    "\t- What does this imply about sensor reliability? -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f72e6f",
   "metadata": {},
   "source": [
    "### Part 4 — Final Reflection (15 pts)\n",
    "\n",
    "1.\tWrite a brief (150–200 words) conclusion summarizing (5 pts)\n",
    "2.\tIn real-world spam application, how would you deal with data distribution related challenges? (5 pts)\n",
    "3.\tWhat further analyses or models would you explore with more time or resources? (5 pts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4hc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
